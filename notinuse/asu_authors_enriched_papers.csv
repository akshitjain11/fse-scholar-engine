author,title,year,link,ss_title,ss_url,ss_venue,ss_year,ss_authors,ss_abstract
Huan Liu,Feature selection for classification,1997,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:u5HHmVD_uO8C,,,,,,
Huan Liu,Fake news detection on social media: A data mining perspective,2017,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:xlqdyEAzORMC,,,,,,
Huan Liu,Feature selection: A data perspective,2017,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:H08txkXQ-cAC,Exploring Large Language Models for Feature Selection: A Data-centric Perspective,https://www.semanticscholar.org/paper/5cb8f11c36acb61ebe4aa8d6ab1c442d5ad7c0f3,SIGKDD Explorations,2024,"Dawei Li, Zhen Tan, Huan Liu","The rapid advancement of Large Language Models (LLMs) has significantly influenced various domains, leveraging their exceptional few-shot and zero-shot learning capabilities. In this work, we aim to explore and understand the LLMs-based feature selection methods from a data-centric perspective. We begin by categorizing existing feature selection methods with LLMs into two groups: data-driven feature selection which requires numerical values of samples to do statistical inference and text-based feature selection which utilizes prior knowledge of LLMs to do semantical associations using descriptive context. We conduct experiments in both classification and regression tasks with LLMs in various sizes (e.g., GPT-4, ChatGPT, and LLaMA-2). Our findings emphasize the effectiveness and robustness of text-based feature selection methods and showcase their potentials using a real-world medical application. We also discuss the challenges and future opportunities in employing LLMs for feature selection, offering insights for further research and development in this emerging field."
Huan Liu,Cross-validation,2009,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:XvxMoLDsR5gC,Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC,https://www.semanticscholar.org/paper/cbc2a612e973491f1b49dffad465c94eed03108d,Statistics and computing,2015,"Aki Vehtari, A. Gelman, Jonah Gabry","Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan."
Huan Liu,Toward integrating feature selection algorithms for classification and clustering,2005,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:d1gkVwhDpl0C,Toward integrating feature selection algorithms for classification and clustering,https://www.semanticscholar.org/paper/185e7e2d397478d0d81b89c6f712b1fa7d62c979,IEEE Transactions on Knowledge and Data Engineering,2005,"Huan Liu, Lei Yu",
Huan Liu,Feature selection for high-dimensional data: A fast correlation-based filter solution,2003,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:2osOgNQ5qMEC,Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution,https://www.semanticscholar.org/paper/b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a,International Conference on Machine Learning,2003,"Lei Yu, Huan Liu",
Huan Liu,Feature selection for knowledge discovery and data mining,2012,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:QnvtoSqgJXcC,Feature Selection for Knowledge Discovery and Data Mining,https://www.semanticscholar.org/paper/91632054692c2ad24f5dd64396f36a4c01be165c,,1998,"Ron Kohavi, George H. John",
Huan Liu,Efficient feature selection via analysis of relevance and redundancy,2004,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:9yKSN-GCB0IC,Efficient Feature Selection via Analysis of Relevance and Redundancy,https://www.semanticscholar.org/paper/de3b02b459a37cfc71f9675647eba8d71aa57476,Journal of machine learning research,2004,"Lei Yu, Huan Liu",
Huan Liu,Feature selection for classification: A review,2014,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:UhchJFpsXkYC,,,,,,
Huan Liu,Subspace clustering for high dimensional data: a review,2004,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:qjMakFHDy7sC,,,,,,
Huan Liu,Computational Methods of Feature Selection,2007,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:8d8msizDQcsC,Computational Methods of Feature Selection,https://www.semanticscholar.org/paper/45417badfeb7b9f974b706f265afd66a94bf9060,,,D. Stracuzzi,
Huan Liu,"Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media",2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:QEdBKvMvu1gC,"FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media",https://www.semanticscholar.org/paper/eed1a4b3ec3b6de0fd1f0b8b2ec969b540fe41a0,Big Data,2018,"Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, Huan Liu","Social media has become a popular means for people to consume and share the news. At the same time, however, it has also enabled the wide dissemination of fake news, that is, news with intentionally false information, causing significant negative effects on society. To mitigate this problem, the research of fake news detection has recently received a lot of attention. Despite several existing computational solutions on the detection of fake news, the lack of comprehensive and community-driven fake news data sets has become one of major roadblocks. Not only existing data sets are scarce, they do not contain a myriad of features often required in the study such as news content, social context, and spatiotemporal information. Therefore, in this article, to facilitate fake news-related research, we present a fake news data repository FakeNewsNet, which contains two comprehensive data sets with diverse features in news content, social context, and spatiotemporal information. We present a comprehensive description of the FakeNewsNet, demonstrate an exploratory analysis of two data sets from different perspectives, and discuss the benefits of the FakeNewsNet for potential applications on fake news study on social media."
Huan Liu,Chi2: Feature selection and discretization of numeric attributes,1995,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:YsMSGLbcyi4C,,,,,,
Huan Liu,Discretization: An enabling technique,2002,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:UeHWp8X0CEIC,Discretization: An Enabling Technique,https://www.semanticscholar.org/paper/14ab9b121a6efab3c11c7db82513628013ec9b04,Data mining and knowledge discovery,2002,"Huan Liu, F. Hussain, C. Tan, M. Dash",
Huan Liu,Is the sample good enough? Comparing data from Twitter's streaming API with Twitter's firehose,2013,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:PUVU9ZnMOtMC,,,,,,
Huan Liu,"Feature extraction, construction and selection: A data mining perspective",1998,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:zYLM7Y9cAGgC,"Feature Extraction, Construction and Selection: A Data Mining Perspective",https://www.semanticscholar.org/paper/a3046db7e02e302fe9661074e238be790f52c3b0,,1998,"Huan Liu, Hiroshi Motoda",
Huan Liu,Spectral feature selection for supervised and unsupervised learning,2007,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:Zph67rFs4hoC,Spectral feature selection for supervised and unsupervised learning,https://www.semanticscholar.org/paper/822b444a9c6920ee5e7952a50121990715d4ddd1,International Conference on Machine Learning,2007,"Zheng Zhao, Huan Liu",
Huan Liu,Consistency-based search in feature selection,2003,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:ufrVoPGSRksC,,,,,,
Huan Liu,Social Media Mining: An Introduction,2014,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:68pgvIssxt4C,Social Media Mining: An Introduction,https://www.semanticscholar.org/paper/7a6b0e84893186577f3602691708d77fdb6048f9,,2014,"R. Zafarani, Mohammad Ali Abbasi, Huan Liu",
Huan Liu,A probabilistic approach to feature selection-a filter solution,1996,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Dzf46C8AAAAJ&citation_for_view=Dzf46C8AAAAJ:IjCSPb-OGe4C,A Probabilistic Approach to Feature Selection - A Filter Solution,https://www.semanticscholar.org/paper/7285ee82aa0cde847fafb8b1109dd19dbdc04e35,International Conference on Machine Learning,1996,"Huan Liu, Rudy Setiono",
Subbarao Kambhampati,What we instagram: A first analysis of instagram photo content and user types,2014,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:uDGL6kOW6j0C,,,,,,
Subbarao Kambhampati,Multiresolution path planning for mobile robots,1986,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:u-x6o8ySG0sC,,,,,,
Subbarao Kambhampati,A validation-structure-based theory of plan modification and reuse,1992,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:u5HHmVD_uO8C,A Validation-Structure-Based Theory of Plan Modification and Reuse,https://www.semanticscholar.org/paper/e916298b927099b9eda7e894b287271e5038d91d,Artificial Intelligence,1992,"S. Kambhampati, J. Hendler",
Subbarao Kambhampati,Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change),2022,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:4UtermoNRQAC,,,,,,
Subbarao Kambhampati,Plan explanations as model reconciliation: Moving beyond explanation as soliloquy,2017,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:r7QigD7TRWAC,,,,,,
Subbarao Kambhampati,On the planning abilities of large language models-a critical investigation,2023,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:FuNQ1RLggQIC,On the Planning Abilities of Large Language Models - A Critical Investigation,https://www.semanticscholar.org/paper/dedfe929d182cc3537a9ed765d589b4735ce062a,Neural Information Processing Systems,2023,"Karthik Valmeekam, Matthew Marquez, S. Sreedharan, Subbarao Kambhampati","Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation."
Subbarao Kambhampati,"Position: LLMs Can’t Plan, But Can Help Planning in LLM-Modulo Frameworks",2024,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:pIp0rujYkN4C,,,,,,
Subbarao Kambhampati,Planning as constraint satisfaction: Solving the planning graph by compiling it into CSP,2001,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:IjCSPb-OGe4C,Planning as constraint satisfaction: Solving the planning graph by compiling it into CSP,https://www.semanticscholar.org/paper/44882ed20e489fb0232f845942b347ba712356e3,Artificial Intelligence,2001,"M. Do, S. Kambhampati",
Subbarao Kambhampati,A survey of moving target defenses for network security,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:9nEbuwLZtFEC,A Survey of Moving Target Defenses for Network Security,https://www.semanticscholar.org/paper/8611d33e837ff1ebd963ce88a669e78291399081,IEEE Communications Surveys and Tutorials,2019,"Sailik Sengupta, Ankur Chowdhary, Abdulhakim Sabur, Dijiang Huang, Adel Alshamrani, S. Kambhampati","Network defenses based on traditional tools, techniques, and procedures (TTP) fail to account for the attacker’s inherent advantage present due to the static nature of network services and configurations. To take away this asymmetric advantage, Moving Target Defense (MTD) continuously shifts the configuration of the underlying system, in turn reducing the success rate of cyberattacks. In this survey, we analyze the recent advancements made in the development of MTDs and highlight (1) how these defenses can be defined using common terminology, (2) can be made more effective with the use of artificial intelligence techniques for decision making, (3) be implemented in practice and (4) evaluated. We first define an MTD using a simple and yet general notation that captures the key aspects of such defenses. We then categorize these defenses into different sub-classes depending on what they move, when they move and how they move. In trying to answer the latter question, we showcase the use of domain knowledge and game-theoretic modeling can help the defender come up with effective and efficient movement strategies. Second, to understand the practicality of these defense methods, we discuss how various MTDs have been implemented and find that networking technologies such as Software Defined Networking and Network Function Virtualization act as key enablers for implementing these dynamic defenses. We then briefly highlight MTD test-beds and case-studies to aid readers who want to examine or deploy existing MTD techniques. Third, our survey categorizes proposed MTDs based on the qualitative and quantitative metrics they utilize to evaluate their effectiveness in terms of security and performance. We use well-defined metrics such as risk analysis and performance costs for qualitative evaluation and metrics based on Confidentiality, Integrity, Availability (CIA), attack representation, QoS impact, and targeted threat models for quantitative evaluation. Finally, we show that our categorization of MTDs is effective in identifying novel research areas and highlight directions for future research."
Subbarao Kambhampati,Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change,2023,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:5kgRglCLipYC,,,,,,
Subbarao Kambhampati,Leveraging pre-trained large language models to construct and utilize world models for model-based task planning,2023,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:DRUjQUnLNDcC,,,,,,
Subbarao Kambhampati,Sapa: A multi-objective metric temporal planner,2003,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:UeHWp8X0CEIC,Sapa: A Multi-objective Metric Temporal Planner,https://www.semanticscholar.org/paper/02e212cd3d0c2d0dd1cacdaaddc018fe6fa38484,Journal of Artificial Intelligence Research,2003,"M. Do, S. Kambhampati","SAPA is a domain-independent heuristic forward chaining planner that can handle durative actions, metric resource constraints, and deadline goals. It is designed to be capable of handling the multi-objective nature of metric temporal planning. Our technical contributions include (i) planning-graph based methods for deriving heuristics that are sensitive to both cost and makespan (ii) techniques for adjusting the heuristic estimates to take action interactions and metric resource limitations into account and (iii) a linear time greedy post-processing technique to improve execution flexibility of the solution plans. An implementation of SAPA using many of the techniques presented in this paper was one of the best domain independent planners for domains with metric and temporal constraints in the third International Planning Competition, held at AIPS-02. We describe the technical details of extracting the heuristics and present an empirical evaluation of the current implementation of SAPA."
Subbarao Kambhampati,Planning as refinement search: A unified framework for evaluating design tradeoffs in partial-order planning,1995,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:d1gkVwhDpl0C,Planning as Refinement Search: A Unified Framework for Evaluating Design Tradeoffs in Partial-Order Planning,https://www.semanticscholar.org/paper/ac648a975bd1a4d82ca537053adff73afda44d5c,Artificial Intelligence,1995,"S. Kambhampati, Craig A. Knoblock, Qiang Yang",
Subbarao Kambhampati,When is temporal planning really temporal?,2007,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:dhFuZR0502QC,,,,,,
Subbarao Kambhampati,Integration of biological sources: current systems and challenges ahead,2004,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:9yKSN-GCB0IC,Integration of biological sources: current systems and challenges ahead,https://www.semanticscholar.org/paper/dd404be16d88e53c4aded2e4fb6d5f92cb5052ce,SGMD,2004,"Thomas Hernandez, S. Kambhampati",
Subbarao Kambhampati,Reviving partial order planning,2001,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:qjMakFHDy7sC,,,,,,
Subbarao Kambhampati,The emerging landscape of explainable ai planning and decision making,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:m1aD9PlKDecC,The Emerging Landscape of Explainable AI Planning and Decision Making,https://www.semanticscholar.org/paper/d1f9a55d0c03d13513048513b39f48dd77deb619,arXiv.org,2020,"Tathagata Chakraborti, S. Sreedharan, S. Kambhampati",
Subbarao Kambhampati,Plan explicability and predictability for robot task planning,2017,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:CoqsOaBEKcQC,Plan explicability and predictability for robot task planning,https://www.semanticscholar.org/paper/98e0d0e31f44d35116d5685119b9f6f9741d3825,IEEE International Conference on Robotics and Automation,2015,"Yu Zhang, S. Sreedharan, Anagha Kulkarni, Tathagata Chakraborti, Hankui Zhuo, S. Kambhampati","Intelligent robots and machines are becoming pervasive in human populated environments. A desirable capability of these agents is to respond to goal-oriented commands by autonomously constructing task plans. However, such autonomy can add significant cognitive load and potentially introduce safety risks to humans when agents behave in unexpected ways. Hence, for such agents to be helpful, one important requirement is for them to synthesize plans that can be easily understood by humans. While there exists previous work that studied socially acceptable robots that interact with humans in “natural ways”, and work that investigated legible motion planning, there is no general solution for high level task planning. To address this issue, we introduce the notions of plan explicability and predictability. To compute these measures, first, we postulate that humans understand agent plans by associating abstract tasks with agent actions, which can be considered as a labeling process. We learn the labeling scheme of humans for agent plans from training examples using conditional random fields (CRFs). Then, we use the learned model to label a new plan to compute its explicability and predictability. These measures can be used by agents to proactively choose or directly synthesize plans that are more explicable and predictable to humans. We provide evaluations on a synthetic domain and with a physical robot to demonstrate the effectiveness of our approach."
Subbarao Kambhampati,# metoo through the lens of social media,2018,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:sFUlmsclzkgC,,,,,,
Subbarao Kambhampati,Explicability? legibility? predictability? transparency? privacy? security? the emerging landscape of interpretable agent behavior,2019,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=yl3L07sAAAAJ&citation_for_view=yl3L07sAAAAJ:S1WaUgt8gYIC,Explicability? Legibility? Predictability? Transparency? Privacy? Security? The Emerging Landscape of Interpretable Agent Behavior,https://www.semanticscholar.org/paper/e4cd356bc95e98328be8928a2c1cce53e7976911,International Conference on Automated Planning and Scheduling,2018,"Tathagata Chakraborti, Anagha Kulkarni, S. Sreedharan, David E. Smith, S. Kambhampati","There has been significant interest of late in generating behavior of agents that is interpretable to the human (observer) in the loop. However, the work in this area has typically lacked coherence on the topic, with proposed solutions for “explicable”, “legible”, “predictable” and “transparent” planning with overlapping, and sometimes conflicting, semantics all aimed at some notion of understanding what intentions the observer will ascribe to an agent by observing its behavior. This is also true for the recent works on “security” and “privacy” of plans which are also trying to answer the same question, but from the opposite point of view – i.e. when the agent is trying to hide instead of reveal its intentions. This paper attempts to provide a workable taxonomy of relevant concepts in this exciting and emerging field of inquiry."
Yezhou Yang,Neural style transfer: A review,2019,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:ALROH1vI_8AC,Neural Style Transfer: A Review,https://www.semanticscholar.org/paper/b0760764dc573b519f76d5a79531d49af333c67a,IEEE Transactions on Visualization and Computer Graphics,2017,"Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Mingli Song","The seminal work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNNs) in creating artistic imagery by separating and recombining image content and style. This process of using CNNs to render a content image in different styles is referred to as Neural Style Transfer (NST). Since then, NST has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention and a variety of approaches are proposed to either improve or extend the original NST algorithm. In this paper, we aim to provide a comprehensive overview of the current progress towards NST. We first propose a taxonomy of current algorithms in the field of NST. Then, we present several evaluation methods and compare different NST algorithms both qualitatively and quantitatively. The review concludes with a discussion of various applications of NST and open problems for future research. A list of papers discussed in this review, corresponding codes, pre-trained models and more comparison results are publicly available at: https://osf.io/f8tu4/."
Yezhou Yang,Corpus-guided sentence generation of natural images,2011,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:W7OEmFMy1HYC,Corpus-Guided Sentence Generation of Natural Images,https://www.semanticscholar.org/paper/76a1dca3a9c2b0229c1b12c95752dcf40dc95a11,Conference on Empirical Methods in Natural Language Processing,2011,"Yezhou Yang, C. L. Teo, Hal Daumé, Y. Aloimonos",
Yezhou Yang,Robot Learning Manipulation Action Plans by “Watching” Unconstrained Videos from the World Wide Web,2015,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:_axFR9aDTf0C,"Robot Learning Manipulation Action Plans by ""Watching"" Unconstrained Videos from the World Wide Web",https://www.semanticscholar.org/paper/e2b593ec292395182f5f6556d47bf3d38ec7e191,AAAI Conference on Artificial Intelligence,2015,"Yezhou Yang, Yi Li, C. Fermüller, Y. Aloimonos","
 
 In order to advance action generation and creation in robots beyond simple learned schemas we need computational tools that allow us to automatically interpret and represent human actions. This paper presents a system that learns manipulation action plans by processing unconstrained videos from the World Wide Web. Its goal is to robustly generate the sequence of atomic actions of seen longer actions in video in order to acquire knowledge for robots. The lower level of the system consists of two convolutional neural network (CNN) based recognition modules, one for classifying the hand grasp type and the other for object recognition. The higher level is a probabilistic manipulation action grammar based parsing module that aims at generating visual sentences for robot manipulation. Experiments conducted on a publicly available unconstrained video dataset show that the system is able to learn manipulation actions by ``watching'' unconstrained videos with high accuracy.
 
"
Yezhou Yang,ViTAA: Visual-Textual Attributes Alignment in Person Search by Natural Language,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:1yWc8FF-_SYC,ViTAA: Visual-Textual Attributes Alignment in Person Search by Natural Language,https://www.semanticscholar.org/paper/03a64b94882f3292ec81c3cf153cb2d7eebc509b,European Conference on Computer Vision,2020,"Zhe Wang, Zhiyuan Fang, Jun Wang, Yezhou Yang","Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches the given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as the performance boosting by a robust feature learning that the referred identity can be accurately bundled by multiple attribute visual cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into subspaces corresponding to attributes using a light auxiliary attribute segmentation computing branch. It then aligns these visual features with the textual attributes parsed from the sentences by using a novel contrastive learning loss. Upon that, we validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Code will be publicly available upon publication."
Yezhou Yang,Transductive Unbiased Embedding for Zero-Shot Learning,2018,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:HeT0ZceujKMC,,,,,,
Yezhou Yang,SEED: Self-supervised Distillation For Visual Representation,2021,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:Br1UauaknNIC,,,,,,
Yezhou Yang,Image Understanding using Vision and Reasoning through Scene Description Graph,2017,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:bKqednn6t2AC,Image Understanding using vision and reasoning through Scene Description Graph,https://www.semanticscholar.org/paper/e1e2e32f29cf7d23881e98dfe018d9049bdb070d,Computer Vision and Image Understanding,2018,"Somak Aditya, Yezhou Yang, Chitta Baral, Y. Aloimonos, C. Fermüller",
Yezhou Yang,MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:WHdLCjDvYFkC,,,,,,
Yezhou Yang,Injecting Semantic Concepts into End-to-End Image Captioning,2022,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:kWvqk_afx_IC,Injecting Semantic Concepts into End-to-End Image Captioning,https://www.semanticscholar.org/paper/45348358505da4158afb98e0e18ee4e384d8d798,Computer Vision and Pattern Recognition,2021,"Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lin Liang, Zhe Gan, Lijuan Wang, Yezhou Yang, Zicheng Liu","Tremendous progresses have been made in recent years in developing better image captioning models, yet most of them rely on a separate object detector to extract regional features. Recent vision-language studies are shifting towards the detector-free trend by leveraging grid representations for more flexible model training and faster inference speed. However, such development is primarily focused on image understanding tasks, and remains less investigated for the caption generation task. In this paper, we are concerned with a better-performing detector-free image captioning model, and propose a pure vision transformer-based image captioning model, dubbed as ViTCAP, in which grid representations are used without extracting the regional features. For improved performance, we introduce a novel Concept Token Network (CTN) to predict the semantic concepts and then incorporate them into the end-to-end captioning. In particular, the CTN is built on the basis of a vision transformer, and is designed to predict the concept tokens through a classification task, from which the rich semantic information contained greatly benefits the captioning task. Compared with the previous detector-based models, ViTCAP drastically simplifies the architectures and at the same time achieves competitive performance on various challenging image captioning datasets. In particular, ViTCAP reaches 138.1 CIDEr scores on COCO-caption Karpathy-split, 93.8 and 108.6 CIDEr scores on nocaps and Google-CC captioning datasets, respectively."
Yezhou Yang,Stroke controllable fast style transfer with adaptive receptive fields,2018,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:yMeIxYmEMEAC,Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields,https://www.semanticscholar.org/paper/357613f3d6c3db33e91654ecef2dff701251a2de,European Conference on Computer Vision,2018,"Yongcheng Jing, Yang Liu, Yezhou Yang, Zunlei Feng, Yizhou Yu, Mingli Song","Recently, in the community of Neural Style Transfer, several algorithms are proposed to transfer an artistic style in real-time, which is known as Fast Style Transfer. However, controlling the stroke size in stylized results still remains an open challenge. To achieve controllable stroke sizes, several attempts were made including training multiple models and resizing the input image in a variety of scales, respectively. However, their results are not promising regarding the efficiency and quality. In this paper, we present a stroke controllable style transfer network that incorporates different stroke sizes into one single model. Firstly, by analyzing the factors that influence the stroke size, we adopt the idea that both the receptive field and the style image scale should be taken into consideration for most cases. Then we propose a StrokePyramid module to endow the network with adaptive receptive fields, and two training strategies to achieve faster convergence and augment new stroke sizes upon a trained model respectively. Finally, by combining the proposed runtime control techniques, our network can produce distinct stroke sizes in different output images or different spatial regions within the same output image. The experimental results demonstrate that with almost the same number of parameters as the previous Fast Style Transfer algorithm, our network can transfer an artistic style in a stroke controllable manner."
Yezhou Yang,Active Adversarial Evader Tracking with a Probabilistic Pursuer under the Pursuit-Evasion Game Framework,2019,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:6yz0xqPARnAC,Active Adversarial Evader Tracking with a Probabilistic Pursuer under the Pursuit-Evasion Game Framework,https://www.semanticscholar.org/paper/4e5cdf40d81f942c31f84e0036569f9123c3ea90,arXiv.org,2019,"V. Jammula, Anshul Rai, Yezhou Yang","Given a mapped environment, we formulate the problem of visually tracking and following an evader using a probabilistic framework. In this work, we consider a non-holonomic robot with a limited visibility depth sensor in an indoor environment with obstacles. The mobile robot that follows the target is considered a pursuer and the agent being followed is considered an evader. We propose a probabilistic framework for both the pursuer and evader to achieve their conflicting goals. We introduce a smart evader that has information about the location of the pursuer. The goal of this variant of the evader is to avoid being tracked by the pursuer by using the visibility region information obtained from the pursuer, to further challenge the proposed smart pursuer. To validate the efficiency of the framework, we conduct several experiments in simulation by using Gazebo and evaluate the success rate of tracking an evader in various environments with different pursuer to evader speed ratios. Through our experiments we validate our hypothesis that a smart pursuer tracks an evader more effectively than a pursuer that just navigates in the environment randomly. We also validate that an evader that is aware of the actions of the pursuer is more successful at avoiding getting tracked by a smart pursuer than a random evader. Finally, we empirically show that while a smart pursuer does increase it's average success rate of tracking compared to a random pursuer, there is an increased variance in its success rate distribution when the evader becomes aware of its actions."
Yezhou Yang,Compressing Visual-linguistic Model via Knowledge Distillation,2021,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:DBa1UEJaJKAC,,,,,,
Yezhou Yang,VQA-LOL: Visual Question Answering under the Lens of Logic,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:cK4Rrx0J3m0C,VQA-LOL: Visual Question Answering under the Lens of Logic,https://www.semanticscholar.org/paper/20dc158a6abd1f92a4534ae064d527821a91685d,European Conference on Computer Vision,2020,"Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang","Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this \textit{Lens of Logic}, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our {Lens of Logic (LOL)} model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Frechet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding."
Yezhou Yang,Grasp Type Revisited: A Modern Perspective on A Classical Feature for Vision,2015,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:kh2fBNsKQNwC,,,,,,
Yezhou Yang,Benchmarking Spatial Relationships in Text-to-Image Generation,2022,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:DrR-2ekChdkC,Benchmarking Spatial Relationships in Text-to-Image Generation,https://www.semanticscholar.org/paper/4bf77d64b860ed0cd84a63aecd92a3cb295b88ee,arXiv.org,2022,"Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, E. Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang","Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, $\mathrm{SR}_{2D}$, that contains sentences describing two or more objects and the spatial relationships between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations between them. Our analyses demonstrate several biases and artifacts of T2I models such as the difficulty with generating multiple objects, a bias towards generating the first object mentioned, spatially inconsistent outputs for equivalent relationships, and a correlation between object co-occurrence and spatial understanding capabilities. We conduct a human study that shows the alignment between VISOR and human judgement about spatial understanding. We offer the $\mathrm{SR}_{2D}$ dataset and the VISOR metric to the community in support of T2I reasoning research."
Yezhou Yang,Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:MpfHP-DdYjUC,,,,,,
Yezhou Yang,Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering,2018,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:DJbcl8HfkQkC,,,,,,
Yezhou Yang,A Cognitive System for Understanding Human Manipulation Actions,2014,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:hCrLmN-GePgC,A Cognitive System for Understanding Human Manipulation Actions,https://www.semanticscholar.org/paper/fcfde425a2315d4c7c53042b54f83b1cc7f4c790,,2014,"Yezhou Yang, Anupam Guha, C. Fermüller, Y. Aloimonos, A. Williams",
Yezhou Yang,Detection of Manipulation Action Consequences (MAC),2013,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:foquWX3nUaYC,,,,,,
Yezhou Yang,Integrating Knowledge and Reasoning in Image Understanding,2019,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=k2suuZgAAAAJ&citation_for_view=k2suuZgAAAAJ:4xDN1ZYqzskC,,,,,,
Chitta Baral,"Knowledge representation, reasoning and declarative problem solving",2003,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:u5HHmVD_uO8C,"Knowledge Representation, Reasoning and Declarative Problem Solving",https://www.semanticscholar.org/paper/7ffc83869f8d32e2bfa83e4bd7afcf5ff8c92a4b,,2003,Chitta Baral,
Chitta Baral,Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,2023,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:08Fspu5RUkwC,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881,arXiv.org,2022,"Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, A. Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, A. La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, A. Tabassum, Arul Menezes, Arun Kirubarajan, A. Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs, B. R. Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, B. Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C'esar Ferri Ram'irez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, D. Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, D. Gonz'alez, Danielle R. Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, D. Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, E. D. Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodolà, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, E. Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart'inez-Plumed, Francesca Happ'e, François Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-L'opez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Schutze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jaehoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Koco'n, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Narain Sohl-Dickstein, Jason Phang, Jason Wei, J. Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Jane W Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jorg Frohberg, Jos Rozen, J. Hernández-Orallo, Joseph Boudeman, J. Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, K. Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, K. Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-philippe Morency, Luca Moschella, Luca Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col'on, Luke Metz, Lutfi Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ram’irez Quintana, M. Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, M. Schubert, Medina Baitemirova, Melody Arnaud, M. McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, M. Strube, Michal Swkedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Monica Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, T. MukundVarma, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, N. Keskar, Niveditha Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, P. Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, P. Eckersley, Phu Mon Htut, P. Hwang, P. Milkowski, P. Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphael Milliere, Rhythm Garg, Richard Barnes, R. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, R. Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, R. Teehan, Rylan Yang, Sahib Singh, Saif Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi S. Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, S. Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, S. Melzi, Siva Reddy, S. Makini, Soo-Hwan Lee, Spencer Bradley Torene, Sriharsha Hatwar, S. Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T Piantadosi, Stuart M. Shieber, Summer Misherghi, S. Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, T. Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, T. Kornev, T. Tunduny, Tobias Gerstenberg, T. Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, V. Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, W. Fedus, W. Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu","Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit""breakthrough""behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting."
Chitta Baral,Cross-task generalization via natural language crowdsourcing instructions,2021,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:JMEA1obkRKoC,Cross-Task Generalization via Natural Language Crowdsourcing Instructions,https://www.semanticscholar.org/paper/cbdb45fc16b0885905b91d84281c310e6cb49e9c,Annual Meeting of the Association for Computational Linguistics,2021,"Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi","Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction."
Chitta Baral,Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks,2022,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:0IGl3jR3yJEC,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0,Conference on Empirical Methods in Natural Language Processing,2022,"Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, H. Lai, I. Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi","How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models."
Chitta Baral,Logic programming and knowledge representation,1994,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:u-x6o8ySG0sC,,,,,,
Chitta Baral,Probabilistic reasoning with answer sets,2009,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:hqOjcs7Dif8C,Under Consideration for Publication in Theory and Practice of Logic Programming Probabilistic Reasoning with Answer Sets,https://www.semanticscholar.org/paper/efd91a5f392a761a7ec119fee8ee1d2c80403a24,,2005,"Chitta Baral, M. Gelfond, J. N. Rushton",
Chitta Baral,COMBINING KNOWLEDGE BASES CONSISTING OF FIRST‐ORDER THEORIES,1992,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:d1gkVwhDpl0C,COMBINING KNOWLEDGE BASES CONSISTING OF FIRST‐ORDER THEORIES,https://www.semanticscholar.org/paper/b512dff98695c253c5cec965c9487607b8d18503,International Conference on Climate Informatics,1991,"Chitta Baral, Sarit Kraus, J. Minker, V. S. Subrahmanian",
Chitta Baral,Combining multiple knowledge bases,1991,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:9yKSN-GCB0IC,INTEGRA: a web-based differential diagnosis system combining multiple knowledge bases,https://www.semanticscholar.org/paper/f38f4be3471ae33c2f7081d9ca27f9097d7b8105,Petra,2020,"Aris Papakonstantinou, H. Kondylakis, Emmanouil I. Marakakis",
Chitta Baral,Reframing instructional prompts to gptk's language,2021,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:KjnAay3C9J8C,,,,,,
Chitta Baral,Language-conditioned imitation learning for robot manipulation tasks,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:ypzvKOdbExQC,Language-Conditioned Imitation Learning for Robot Manipulation Tasks,https://www.semanticscholar.org/paper/7ca4abace88db259faed67686ed7bba02b46eb82,Neural Information Processing Systems,2020,"Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, H. B. Amor","Imitation learning is a popular approach for teaching motor skills to robots. However, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., ""go to the large green bowl""). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more fine-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods."
Chitta Baral,What to do and how to do it: Translating natural language directives into temporal and dynamic logic representation for goal management and action execution,2009,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:Wp0gIr-vW9MC,,,,,,
Chitta Baral,Reasoning agents in dynamic domains,2000,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:qjMakFHDy7sC,Reasoning agents in dynamic domains,https://www.semanticscholar.org/paper/74f4afbef51d69ad45e1d4e97cba0a12edf1c514,,2000,"Chitta Baral, M. Gelfond",
Chitta Baral,Discovering drug–drug interactions: a text-mining and reasoning approach based on properties of drug metabolism,2010,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:M05iB0D1s5AC,,,,,,
Chitta Baral,Computational complexity of planning and approximate planning in the presence of incompleteness,2000,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:zYLM7Y9cAGgC,,,,,,
Chitta Baral,"Representing actions: Laws, observations and hypotheses",1997,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:2osOgNQ5qMEC,,,,,,
Chitta Baral,Formalizing sensing actions—A transition function based approach,2001,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:IjCSPb-OGe4C,Formalizing sensing actions A transition function based approach,https://www.semanticscholar.org/paper/234c2cca8f8cf2aa23c9cf90412f7253f8b09b0b,Artificial Intelligence,2001,"Tran Cao Son, Chitta Baral",
Chitta Baral,Mutant: A training paradigm for out-of-distribution generalization in visual question answering,2020,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:jtusTj6o6osC,,,,,,
Chitta Baral,Learning to use formulas to solve simple arithmetic problems,2016,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:t1niNHmIXQYC,Learning To Use Formulas To Solve Simple Arithmetic Problems,https://www.semanticscholar.org/paper/f6b5335f27b9583dd152d8cd4ea9134e24bd297b,Annual Meeting of the Association for Computational Linguistics,2016,"Arindam Mitra, Chitta Baral","Solving simple arithmetic word problems is one of the challenges in Natural Language Understanding. This paper presents a novel method to learn to use formulas to solve simple arithmetic word problems. Our system, analyzes each of the sentences to identify the variables and their attributes; and automatically maps this information into a higher level representation. It then uses that representation to recognize the presence of a formula along with its associated variables. An equation is then generated from the formal description of the formula. In the training phase, it learns to score the pair from the systematically generated higher level representation. It is able to solve 86.07% of the problems in a corpus of standard primary school test questions and beats the state-of-the-art by"
Chitta Baral,Dualities between alternative semantics for logic programming and nonmonotonic reasoning,1993,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:UeHWp8X0CEIC,Dualities between alternative semantics for logic programming and nonmonotonic reasoning,https://www.semanticscholar.org/paper/97a0551f90e6c62064ce711b53d42aedc419c3c3,Journal of automated reasoning,1991,"Chitta Baral, V. S. Subrahmanian",
Chitta Baral,Lila: A unified benchmark for mathematical reasoning,2022,https://scholar.google.com/citations?view_op=view_citation&hl=en&user=9Yd716IAAAAJ&citation_for_view=9Yd716IAAAAJ:UbXTy9l1WKIC,LILA: A Unified Benchmark for Mathematical Reasoning,https://www.semanticscholar.org/paper/52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7,Conference on Empirical Methods in Natural Language Processing,2022,"Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, S. Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, A. Kalyan","Mathematical reasoning skills are essential for general-purpose intelligentsystems to perform tasks from grocery shopping to climate modeling.Towards evaluating and improving AI systems in this domain, we proposeLILA, a unified mathematical reasoning benchmark consisting of 23 diversetasks along four dimensions:(i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs,thereby obtaining explainable solutions in addition to the correct answer.We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation.Finally, we introduce BHASKARA,a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models),while the best performing model only obtains 60.40%,indicating the room for improvement in general mathematical reasoning and understanding."
